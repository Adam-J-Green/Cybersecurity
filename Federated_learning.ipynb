{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQUvYqF8vhPAv4Y+MZZh1r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JollygreenG-10/Cybersecurity/blob/main/Federated_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OxnNlS8CJxTy"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet tensorflow-federated\n",
        "!pip install --quiet --upgrade dp-accounting\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_federated as tff\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_full = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
        "data_full = pd.DataFrame(data_full)\n",
        "data_full.columns = data_full.columns.str.replace(\" \", \"\")"
      ],
      "metadata": {
        "id": "4-XTS7vSmUY7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_full.info()\n",
        "data_full['Label'].value_counts()\n",
        "data_full.describe()"
      ],
      "metadata": {
        "id": "PfQ_--fNB_OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_dict = {'BENIGN':0, 'DDoS':1}\n",
        "data_full['Label'] = data_full['Label'].map(mapping_dict)\n",
        "data_full = data_full.sample(frac = 1)\n",
        "labels = data_full['Label']\n",
        "data_full.drop(['Label', 'FlowBytes/s', 'FlowPackets/s'], axis = 1, inplace = True)\n",
        "\n",
        "for col in data_full.columns:\n",
        "  scaler = MinMaxScaler()\n",
        "  try:\n",
        "    data_full[col] = scaler.fit_transform(data_full[[col]])\n",
        "  except:\n",
        "    print(col)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data_full, labels, test_size = 0.2, random_state = 10)\n"
      ],
      "metadata": {
        "id": "EXuG9O84oT5s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build(shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(shape, input_shape=(shape,)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dense(50))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dense(50))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "    return model\n",
        "\n",
        "lr = 0.0001\n",
        "comms_round = 100\n",
        "loss=tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = ['accuracy', 'Precision', 'Recall']"
      ],
      "metadata": {
        "id": "dN7riLLqlzMl"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train).astype('float')\n",
        "y_train = np.array(y_train)\n",
        "SGD_model = build(x_train.shape[1])\n",
        "\n",
        "SGD_model.compile(loss=loss,\n",
        "              optimizer=Adam(),\n",
        "              metrics=metrics)\n",
        "\n",
        "# fit the SGD training data to model\n",
        "fitted_model = SGD_model.fit(x_train, y_train, epochs=50, batch_size=100)"
      ],
      "metadata": {
        "id": "a1zlkuoMW1jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Federated Learning Model 1\n"
      ],
      "metadata": {
        "id": "5xuO8QYsk1ja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create nodes for separation of data"
      ],
      "metadata": {
        "id": "yggFbnYi1OV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    @staticmethod\n",
        "    def build(shape):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(200, input_shape=(shape,)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(200))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(1, activation = 'sigmoid'))\n",
        "        return model\n",
        "\n",
        "lr = 0.0001\n",
        "comms_round = 1\n",
        "loss=tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = ['accuracy', 'Precision', 'Recall']"
      ],
      "metadata": {
        "id": "Q1elLjxHYTkj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_nodes(data, labels, num_nodes: int):\n",
        "  init = 'Node'\n",
        "  node_names = [init+str(i+1) for i in range(num_nodes)]\n",
        "  data = np.array(data)\n",
        "\n",
        "  data = list(zip(data, labels))\n",
        "  random.shuffle(data)\n",
        "\n",
        "  shard_size = len(data)//num_nodes\n",
        "\n",
        "  shards = [data[i:i+shard_size] for i in range(0, shard_size*num_nodes, shard_size)]\n",
        "  assert(len(shards) == num_nodes)\n",
        "\n",
        "  return {node_names[i] : shards[i] for i in range(len(node_names))}\n",
        "\n",
        "nodes = create_nodes(x_train, y_train, 50)\n",
        "\n",
        "def batch_data(shard):\n",
        "  data, label = zip(*shard)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((np.array(data), np.array(label)))\n",
        "  return dataset.shuffle(len(label)).batch(32)\n",
        "\n"
      ],
      "metadata": {
        "id": "KMEq6vBM1M6k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process and batch the training data for each client\n",
        "nodes_batched = dict()\n",
        "for (node_name, data) in nodes.items():\n",
        "\n",
        "  nodes_batched[node_name] = batch_data(data)\n",
        "\n",
        "#process and batch the test set\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(len(y_test))"
      ],
      "metadata": {
        "id": "E-QmP8t01MxY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import normal\n",
        "def weight_scaling_factor(node_train, node_name):\n",
        "    node_names = list(node_train.keys())\n",
        "    #get the bs\n",
        "    bs = list(node_train[node_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(node_train[node_name]).numpy() for node_name in node_names])*bs\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(node_train[node_name]).numpy()*bs\n",
        "    return local_count/global_count\n",
        "\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    average_weights = list()\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        average_weights.append(layer_mean)\n",
        "    return average_weights\n",
        "\n",
        "def test_model(x_test, y_test,  model):\n",
        "    pred = model.predict(x_test).round(1).astype(int)\n",
        "    print(pred)\n",
        "    acc = accuracy_score(y_test, pred)\n",
        "    prec = precision_score(y_test, pred)\n",
        "    recall = recall_score(y_test, pred)\n",
        "    f1 = f1_score(y_test, pred)\n",
        "    print(f'Global Model Accuracy is {acc}, precision is {prec}, recall is {recall}, f1 is {f1}')\n",
        "    return"
      ],
      "metadata": {
        "id": "v_BT_jQPPmN-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#initialize global model\n",
        "smlp_global = MLP()\n",
        "global_model = smlp_global.build(76)\n",
        "\n",
        "#commence global training loop\n",
        "for comm_round in range(1):\n",
        "\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    #initial list to collect local model weights after scaling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    node_names= list(nodes_batched.keys())\n",
        "    random.shuffle(node_names)\n",
        "\n",
        "    #loop through each client and create new local model\n",
        "    for node in node_names:\n",
        "        smlp_local = MLP()\n",
        "        local_model = smlp_local.build(76)\n",
        "        local_model.compile(loss=loss,\n",
        "                      optimizer=tf.keras.optimizers.Adam(),\n",
        "                      metrics=metrics)\n",
        "\n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        #fit local model with client's data\n",
        "        local_model.fit(nodes_batched[node], epochs=3)\n",
        "\n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scaling_factor(nodes_batched, node)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    #update global model\n",
        "    global_model.set_weights(average_weights)"
      ],
      "metadata": {
        "id": "56G4Y3YAQgDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5c008e-cc7f-44c8-d5eb-c95712f7433e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1691 - accuracy: 0.9568 - precision: 0.9413 - recall: 0.9842\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.9875 - precision: 0.9801 - recall: 0.9980\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.9900 - precision: 0.9844 - recall: 0.9980\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1734 - accuracy: 0.9579 - precision: 0.9415 - recall: 0.9868\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 0.9856 - precision: 0.9765 - recall: 0.9985\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0324 - accuracy: 0.9884 - precision: 0.9817 - recall: 0.9980\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 0.1697 - accuracy: 0.9568 - precision: 0.9436 - recall: 0.9830\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9864 - precision: 0.9772 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0250 - accuracy: 0.9900 - precision: 0.9837 - recall: 0.9990\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 5ms/step - loss: 0.1777 - accuracy: 0.9405 - precision: 0.9163 - recall: 0.9854\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0379 - accuracy: 0.9867 - precision: 0.9781 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0333 - accuracy: 0.9881 - precision: 0.9809 - recall: 0.9985\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 2ms/step - loss: 0.1718 - accuracy: 0.9562 - precision: 0.9393 - recall: 0.9874\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0335 - accuracy: 0.9875 - precision: 0.9787 - recall: 1.0000\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.9909 - precision: 0.9857 - recall: 0.9985\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 5ms/step - loss: 0.1790 - accuracy: 0.9535 - precision: 0.9395 - recall: 0.9821\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0462 - accuracy: 0.9850 - precision: 0.9764 - recall: 0.9981\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.0367 - accuracy: 0.9870 - precision: 0.9796 - recall: 0.9981\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1724 - accuracy: 0.9479 - precision: 0.9241 - recall: 0.9903\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9878 - precision: 0.9796 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.9898 - precision: 0.9829 - recall: 0.9995\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1750 - accuracy: 0.9532 - precision: 0.9365 - recall: 0.9867\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.9884 - precision: 0.9809 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0276 - accuracy: 0.9906 - precision: 0.9841 - recall: 1.0000\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 4ms/step - loss: 0.1769 - accuracy: 0.9513 - precision: 0.9315 - recall: 0.9874\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 0.0382 - accuracy: 0.9870 - precision: 0.9782 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9878 - precision: 0.9810 - recall: 0.9981\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1688 - accuracy: 0.9590 - precision: 0.9455 - recall: 0.9837\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0353 - accuracy: 0.9881 - precision: 0.9811 - recall: 0.9980\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.9909 - precision: 0.9849 - recall: 0.9990\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1793 - accuracy: 0.9471 - precision: 0.9270 - recall: 0.9861\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9853 - precision: 0.9761 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9864 - precision: 0.9784 - recall: 0.9986\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1794 - accuracy: 0.9499 - precision: 0.9267 - recall: 0.9903\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0414 - accuracy: 0.9850 - precision: 0.9753 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.9864 - precision: 0.9781 - recall: 0.9985\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 2ms/step - loss: 0.1707 - accuracy: 0.9576 - precision: 0.9393 - recall: 0.9887\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 0.9850 - precision: 0.9760 - recall: 0.9980\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 0.9873 - precision: 0.9797 - recall: 0.9980\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 4ms/step - loss: 0.1674 - accuracy: 0.9587 - precision: 0.9399 - recall: 0.9913\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9870 - precision: 0.9778 - recall: 1.0000\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 0.0312 - accuracy: 0.9875 - precision: 0.9800 - recall: 0.9985\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 3ms/step - loss: 0.1742 - accuracy: 0.9513 - precision: 0.9304 - recall: 0.9866\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9856 - precision: 0.9758 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.9881 - precision: 0.9805 - recall: 0.9985\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 2ms/step - loss: 0.1695 - accuracy: 0.9518 - precision: 0.9302 - recall: 0.9887\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0407 - accuracy: 0.9864 - precision: 0.9774 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9878 - precision: 0.9802 - recall: 0.9985\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1667 - accuracy: 0.9632 - precision: 0.9500 - recall: 0.9862\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0324 - accuracy: 0.9900 - precision: 0.9830 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9922 - precision: 0.9873 - recall: 0.9990\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1637 - accuracy: 0.9657 - precision: 0.9518 - recall: 0.9880\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9873 - precision: 0.9785 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0310 - accuracy: 0.9895 - precision: 0.9828 - recall: 0.9985\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 4ms/step - loss: 0.1579 - accuracy: 0.9670 - precision: 0.9584 - recall: 0.9843\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0344 - accuracy: 0.9878 - precision: 0.9798 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.9911 - precision: 0.9845 - recall: 1.0000\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 3ms/step - loss: 0.1636 - accuracy: 0.9629 - precision: 0.9522 - recall: 0.9826\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0396 - accuracy: 0.9867 - precision: 0.9790 - recall: 0.9975\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.9886 - precision: 0.9814 - recall: 0.9985\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1724 - accuracy: 0.9621 - precision: 0.9525 - recall: 0.9838\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.9867 - precision: 0.9790 - recall: 0.9986\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 0.9884 - precision: 0.9818 - recall: 0.9986\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1645 - accuracy: 0.9651 - precision: 0.9561 - recall: 0.9820\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9873 - precision: 0.9789 - recall: 0.9985\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0349 - accuracy: 0.9878 - precision: 0.9799 - recall: 0.9985\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9482 - precision: 0.9240 - recall: 0.9902\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 0.9853 - precision: 0.9757 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.9881 - precision: 0.9799 - recall: 0.9995\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 4ms/step - loss: 0.1732 - accuracy: 0.9449 - precision: 0.9217 - recall: 0.9896\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0347 - accuracy: 0.9884 - precision: 0.9809 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.9903 - precision: 0.9846 - recall: 0.9991\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 4ms/step - loss: 0.1743 - accuracy: 0.9554 - precision: 0.9436 - recall: 0.9813\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 0.9862 - precision: 0.9774 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.9892 - precision: 0.9843 - recall: 0.9971\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1724 - accuracy: 0.9596 - precision: 0.9434 - recall: 0.9883\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0410 - accuracy: 0.9845 - precision: 0.9740 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 0.9881 - precision: 0.9832 - recall: 0.9961\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1725 - accuracy: 0.9576 - precision: 0.9398 - recall: 0.9894\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0378 - accuracy: 0.9864 - precision: 0.9773 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.9892 - precision: 0.9815 - recall: 1.0000\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1664 - accuracy: 0.9626 - precision: 0.9520 - recall: 0.9833\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 0.9881 - precision: 0.9807 - recall: 0.9985\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.9892 - precision: 0.9812 - recall: 1.0000\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1658 - accuracy: 0.9618 - precision: 0.9474 - recall: 0.9884\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9900 - precision: 0.9834 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.9922 - precision: 0.9872 - recall: 0.9995\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 3s 4ms/step - loss: 0.1607 - accuracy: 0.9629 - precision: 0.9498 - recall: 0.9880\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0332 - accuracy: 0.9878 - precision: 0.9803 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9900 - precision: 0.9835 - recall: 0.9995\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1675 - accuracy: 0.9668 - precision: 0.9541 - recall: 0.9905\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0344 - accuracy: 0.9875 - precision: 0.9794 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.9895 - precision: 0.9831 - recall: 0.9990\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1781 - accuracy: 0.9454 - precision: 0.9215 - recall: 0.9883\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9873 - precision: 0.9786 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.9881 - precision: 0.9823 - recall: 0.9971\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1669 - accuracy: 0.9596 - precision: 0.9498 - recall: 0.9798\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0390 - accuracy: 0.9864 - precision: 0.9778 - recall: 0.9985\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 0.9895 - precision: 0.9835 - recall: 0.9980\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 0.1749 - accuracy: 0.9493 - precision: 0.9279 - recall: 0.9862\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0333 - accuracy: 0.9884 - precision: 0.9806 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0263 - accuracy: 0.9906 - precision: 0.9844 - recall: 0.9990\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 4ms/step - loss: 0.1751 - accuracy: 0.9526 - precision: 0.9545 - recall: 0.9615\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.9839 - precision: 0.9722 - recall: 1.0000\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0319 - accuracy: 0.9875 - precision: 0.9806 - recall: 0.9975\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1747 - accuracy: 0.9502 - precision: 0.9276 - recall: 0.9887\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.9864 - precision: 0.9788 - recall: 0.9975\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.9892 - precision: 0.9821 - recall: 0.9990\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1668 - accuracy: 0.9496 - precision: 0.9275 - recall: 0.9899\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0341 - accuracy: 0.9881 - precision: 0.9802 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.9886 - precision: 0.9834 - recall: 0.9971\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1741 - accuracy: 0.9615 - precision: 0.9443 - recall: 0.9913\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9853 - precision: 0.9750 - recall: 1.0000\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.9878 - precision: 0.9796 - recall: 0.9995\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1801 - accuracy: 0.9385 - precision: 0.9092 - recall: 0.9886\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9867 - precision: 0.9777 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9878 - precision: 0.9800 - recall: 0.9985\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 4ms/step - loss: 0.1654 - accuracy: 0.9610 - precision: 0.9458 - recall: 0.9878\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.9889 - precision: 0.9813 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.9898 - precision: 0.9827 - recall: 0.9995\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1741 - accuracy: 0.9374 - precision: 0.9125 - recall: 0.9855\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0323 - accuracy: 0.9881 - precision: 0.9802 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9920 - precision: 0.9867 - recall: 0.9995\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 2ms/step - loss: 0.1851 - accuracy: 0.9346 - precision: 0.9073 - recall: 0.9886\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9859 - precision: 0.9772 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0346 - accuracy: 0.9867 - precision: 0.9808 - recall: 0.9967\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1677 - accuracy: 0.9610 - precision: 0.9440 - recall: 0.9880\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.9864 - precision: 0.9779 - recall: 0.9980\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.9892 - precision: 0.9823 - recall: 0.9985\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1657 - accuracy: 0.9612 - precision: 0.9494 - recall: 0.9851\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0358 - accuracy: 0.9878 - precision: 0.9802 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0303 - accuracy: 0.9892 - precision: 0.9848 - recall: 0.9966\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 5ms/step - loss: 0.1680 - accuracy: 0.9604 - precision: 0.9480 - recall: 0.9825\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 1s 4ms/step - loss: 0.0360 - accuracy: 0.9878 - precision: 0.9790 - recall: 0.9995\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0330 - accuracy: 0.9881 - precision: 0.9814 - recall: 0.9975\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1742 - accuracy: 0.9496 - precision: 0.9293 - recall: 0.9863\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0408 - accuracy: 0.9848 - precision: 0.9748 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0342 - accuracy: 0.9881 - precision: 0.9822 - recall: 0.9971\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1711 - accuracy: 0.9574 - precision: 0.9526 - recall: 0.9745\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0364 - accuracy: 0.9859 - precision: 0.9770 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.9898 - precision: 0.9830 - recall: 0.9995\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1623 - accuracy: 0.9604 - precision: 0.9539 - recall: 0.9776\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 0.9886 - precision: 0.9818 - recall: 0.9985\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.9900 - precision: 0.9855 - recall: 0.9971\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 1s 3ms/step - loss: 0.1783 - accuracy: 0.9524 - precision: 0.9327 - recall: 0.9879\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9862 - precision: 0.9772 - recall: 0.9990\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0310 - accuracy: 0.9895 - precision: 0.9828 - recall: 0.9990\n",
            "Epoch 1/3\n",
            "113/113 [==============================] - 2s 4ms/step - loss: 0.1783 - accuracy: 0.9418 - precision: 0.9184 - recall: 0.9843\n",
            "Epoch 2/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9845 - precision: 0.9755 - recall: 0.9975\n",
            "Epoch 3/3\n",
            "113/113 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.9873 - precision: 0.9802 - recall: 0.9975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(x_test, y_test, global_model)"
      ],
      "metadata": {
        "id": "NJgBmHe_iN4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Federated Learning with Differential Privacy"
      ],
      "metadata": {
        "id": "v-lSizs_Bx8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(modelweights):\n",
        "  for layer in modelweights:\n",
        "    noise = normal(loc=0.0, scale=1.0, size=layer.shape)\n",
        "    layer += noise\n",
        "  return modelweights"
      ],
      "metadata": {
        "id": "A6MIWNi0c1dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize global model\n",
        "smlp_global = MLP()\n",
        "global_model = smlp_global.build(76)\n",
        "\n",
        "#commence global training loop\n",
        "for comm_round in range(comms_round):\n",
        "\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    #initial list to collect local model weights after scaling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    node_names= list(nodes_batched.keys())\n",
        "    random.shuffle(node_names)\n",
        "\n",
        "    #loop through each client and create new local model\n",
        "    for node in node_names:\n",
        "        smlp_local = MLP()\n",
        "        local_model = smlp_local.build(76)\n",
        "        local_model.compile(loss=loss,\n",
        "                      optimizer=tf.keras.optimizers.Adam(),\n",
        "                      metrics=metrics)\n",
        "\n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        #fit local model with client's data\n",
        "        local_model.fit(nodes_batched[node], epochs=3)\n",
        "\n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scaling_factor(nodes_batched, node)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_weights= add_noise(scaled_weights)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "\n",
        "\n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    #update global model\n",
        "    global_model.set_weights(average_weights)"
      ],
      "metadata": {
        "id": "n2yYaqlP2ByP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}