{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxnNlS8CJxTy",
        "outputId": "25a4352c-90c1-41ce-e5b3-401f2a18cff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.4/114.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.5/558.5 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.2/365.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for farmhashpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.1.2 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet tensorflow-federated\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_federated as tff"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('KDDTrain+.txt')\n",
        "data_test = pd.read_csv('KDDTest+.txt')\n",
        "data_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8jRv5RnhAfa",
        "outputId": "fb4715a3-ddc3-4a3c-8fd9-78f4f8f5d625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['0', 'tcp', 'ftp_data', 'SF', '491', '0.1', '0.2', '0.3', '0.4', '0.5',\n",
              "       '0.6', '0.7', '0.8', '0.9', '0.10', '0.11', '0.12', '0.13', '0.14',\n",
              "       '0.15', '0.16', '0.18', '2', '2.1', '0.00', '0.00.1', '0.00.2',\n",
              "       '0.00.3', '1.00', '0.00.4', '0.00.5', '150', '25', '0.17', '0.03',\n",
              "       '0.17.1', '0.00.6', '0.00.7', '0.00.8', '0.05', '0.00.9', 'normal',\n",
              "       '20'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_full = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
        "data_full = pd.DataFrame(data_full)\n",
        "data_full.columns = data_full.columns.str.replace(\" \", \"\")"
      ],
      "metadata": {
        "id": "4-XTS7vSmUY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = tf.data.Dataset.from_tensor_slices(np.array(data_full))\n"
      ],
      "metadata": {
        "id": "PfQ_--fNB_OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_dict = {'BENIGN':0, 'DDOS':1}\n",
        "data_full['Label'] = data_full['Label'].map(mapping_dict)\n",
        "labels = data_full['Label']\n",
        "data_full.drop(['Label'], axis = 1, inplace = True)\n",
        "x_train, x_test, y_train, y_test = train_test_split(data_full, labels, test_size = 0.2, random_state = 10)"
      ],
      "metadata": {
        "id": "EXuG9O84oT5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "id": "qY-_h68ydjXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create nodes for separation of data"
      ],
      "metadata": {
        "id": "yggFbnYi1OV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_nodes(data, labels, num_nodes: int):\n",
        "  init = 'Node'\n",
        "  node_names = [init+str(i+1) for i in range(num_nodes)]\n",
        "  data = np.array(data)\n",
        "\n",
        "  data = list(zip(data, labels))\n",
        "  random.shuffle(data)\n",
        "\n",
        "  shard_size = len(data)//num_nodes\n",
        "\n",
        "  shards = [data[i:i+shard_size] for i in range(0, shard_size*num_nodes, shard_size)]\n",
        "  print(len(shards))\n",
        "  assert(len(shards) == num_nodes)\n",
        "\n",
        "  return {node_names[i] : shards[i] for i in range(len(node_names))}\n",
        "\n",
        "nodes = create_nodes(x_train, y_train, 50)\n",
        "\n",
        "def batch_data(shard):\n",
        "  data, label = zip(*shard)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((np.array(data), np.array(label)))\n",
        "  return dataset.shuffle(len(label)).batch(32)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMEq6vBM1M6k",
        "outputId": "3f05275c-47d7-4593-998a-fef0d612fc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#process and batch the training data for each client\n",
        "nodes_batched = dict()\n",
        "for (node_name, data) in nodes.items():\n",
        "\n",
        "  print(batch_data(data))\n",
        "  nodes_batched[node_name] = batch_data(data)\n",
        "\n",
        "#process and batch the test set\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(len(y_test))"
      ],
      "metadata": {
        "id": "E-QmP8t01MxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    @staticmethod\n",
        "    def build(shape):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(200, input_shape=(shape,)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(200))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(1))\n",
        "        model.add(Activation(\"softmax\"))\n",
        "        return model\n",
        "\n",
        "lr = 0.01\n",
        "comms_round = 100\n",
        "loss='binary_crossentropy'\n",
        "metrics = ['accuracy']"
      ],
      "metadata": {
        "id": "A0Jh36d2qbYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def weight_scaling_factor(node_train, node_name):\n",
        "    node_names = list(node_train.keys())\n",
        "    #get the bs\n",
        "    bs = list(node_train[node_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(node_train[node_name]).numpy() for node_name in node_names])*bs\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(node_train[node_name]).numpy()*bs\n",
        "    return local_count/global_count\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "\n",
        "    return avg_grad\n",
        "\n",
        "\n",
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "    cce = tf.keras.losses.BinaryCrossentropy()\n",
        "    logits = model.predict(X_test)\n",
        "    print(logits)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss"
      ],
      "metadata": {
        "id": "v_BT_jQPPmN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#initialize global model\n",
        "smlp_global = MLP()\n",
        "global_model = smlp_global.build(78)\n",
        "\n",
        "#commence global training loop\n",
        "for comm_round in range(comms_round):\n",
        "\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    #initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    node_names= list(nodes_batched.keys())\n",
        "    random.shuffle(node_names)\n",
        "\n",
        "    #loop through each client and create new local model\n",
        "    for node in node_names:\n",
        "        smlp_local = MLP()\n",
        "        local_model = smlp_local.build(78)\n",
        "        local_model.compile(loss=loss,\n",
        "                      optimizer=tf.keras.optimizers.Adam(),\n",
        "                      metrics=metrics)\n",
        "\n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        #fit local model with client's data\n",
        "        local_model.fit(nodes_batched[node], epochs=3)\n",
        "\n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scaling_factor(nodes_batched, node)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    #update global model\n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    #test global model and print out metrics after each communications round\n",
        "    for(x_test, y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(x_test, y_test, global_model, comm_round)"
      ],
      "metadata": {
        "id": "56G4Y3YAQgDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for(x_test, y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(x_test, y_test, global_model, comm_round)"
      ],
      "metadata": {
        "id": "T-wElq5BupsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline Model"
      ],
      "metadata": {
        "id": "ob6QXWFEBuJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SGD_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(y_train)).batch(320)\n",
        "smlp_SGD = MLP()\n",
        "SGD_model = smlp_SGD.build(x_train.shape[1])\n",
        "\n",
        "SGD_model.compile(loss=loss,\n",
        "              optimizer=tf.keras.optimizers.Adam,\n",
        "              metrics=metrics)\n",
        "\n",
        "# fit the SGD training data to model\n",
        "fitted_model = SGD_model.fit(SGD_dataset, epochs=100, verbose=0)\n",
        "\n",
        "#test the SGD global model and print out metrics\n",
        "for(X_test, Y_test) in test_batched:\n",
        "        SGD_acc, SGD_loss = test_model(X_test, Y_test, fitted_model, 1)"
      ],
      "metadata": {
        "id": "a1zlkuoMW1jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Federated Learning with Differential Privacy"
      ],
      "metadata": {
        "id": "v-lSizs_Bx8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_emnist_dataset():\n",
        "  emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data(\n",
        "      only_digits=True)\n",
        "\n",
        "  def element_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.expand_dims(element['pixels'], -1), y=element['label'])\n",
        "\n",
        "  def preprocess_train_dataset(dataset):\n",
        "    # Use buffer_size same as the maximum client dataset size,\n",
        "    # 418 for Federated EMNIST\n",
        "    return (dataset.map(element_fn)\n",
        "                   .shuffle(buffer_size=418)\n",
        "                   .repeat(1)\n",
        "                   .batch(32, drop_remainder=False))\n",
        "\n",
        "  def preprocess_test_dataset(dataset):\n",
        "    return dataset.map(element_fn).batch(128, drop_remainder=False)\n",
        "\n",
        "  emnist_train = emnist_train.preprocess(preprocess_train_dataset)\n",
        "  emnist_test = preprocess_test_dataset(\n",
        "      emnist_test.create_tf_dataset_from_all_clients())\n",
        "  return emnist_train, emnist_test\n",
        "\n",
        "train_data, test_data = get_emnist_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EH5WUo_BxDQ",
        "outputId": "c6760169-0413-4ae0-8638-1034fcbcfd4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading emnist_all.sqlite.lzma: 100%|██████████| 170507172/170507172 [01:05<00:00, 2615932.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "6nh7QuHcKuKQ",
        "outputId": "26740196-f8f7-49f0-98dd-bca3df64f0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8852729f0f48>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'PreprocessClientData' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}